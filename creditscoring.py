# -*- coding: utf-8 -*-
"""CreditScoring.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JRLmAapy3s1P7LnOR3J3rbWAY-jwrBqq

# Wprowadzenie

## Cel projektu

Celem projektu jest stworzenie modelu prognozującego indywidualnego ryzyka kredytowego. Zmienna objaśniana jest zmienną binarną klasyfikującą przyjmującą wartości Y- tak przyznano,N - nieprzyznano. W tym celu skorzystam z 3 rodzajów modelów służących do klasyfikacji. Będzie to 

*   regresja logistyczna
*   algorytm K-najbliższych sąsiadów
*  algorytm Support Vector Machine



## Prezentacja danych

Zbiór danych składa się z 13 zmiennych. 

Zmienną objaśnianą w modelu jest zmienna binarna **Loan_Status**, oznaczająca czy udzielono kredytu czy nie wyrażana przez Y,N.

Pierwszą operacją jaką wykonuje jest usunięcie kolumny Loan_ID, ponieważ jest to tylko wartość identyfikująca daną obserwację, nie wprowadza żadnych wartości do modelu.  


Zmiennych objaśniających jest 11 i oznaczają one kolejno: 

*   **Gender** - płeć, zmienna binarna, która przyjmuje wartości Male albo Female
*   **Married** czy osoba starająca się o kredyt jest zamężna, zmienna binarna wartości Yes lub No
*  **Dependents** ilość osób będącyh na utrzymaniu kredytobiorcy, zmienna jakościowa, przyjmuje wartości 0,1,2,+3, gdzie 3+ oznacza 3 lub więcej osób. Zostanie w preprocessingu zamieniona na liczbę 3.
* **Education** , czy kredytobiorca posiada wykształcenie, zmienna binarna przyjmująca wartości Graduate lub Not Graduate 
* **Self_Employed**, czy kredytobiorca jest samozatrudniony, czy nie, zmienna binarna Yes lub No
* **ApplicantIncome** zmienna ilościowa odzwierciedlająca przychód aplikanta
* **CoapplicantIncome** zmienna ilościowa odzwierciedlająca przychód współaplikanta
* **LoanAmount** wysokość kredytu, zmienna ilościowa 
* **Loan_Amount_Term** zmienna ilościowa oznaczająca termin spłąty kredytu
* **Credit_History** zmienna binarna oznaczająca historię kredytu przyjmująca wartości 0,1
* **Property_Area** zmienna jakościowa oznaczająca lokalizację własności przyjmująca wartości  Rural, Urban, Suburban

Ich struktura zostanie przedstawiona poniżej i opisana poniżej przy okazji preprocessingu.
"""

import os
import pandas as pd
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from scipy import stats
import numpy as np
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
data_india = pd.read_csv('kredyt_indie.csv', sep=";")
col_names = list(data_india.columns)
col_names = col_names[1:len(col_names)]
df = data_india[col_names]
df.head()

"""# Data Preprocessing

Zbiór danych składa się z  z 6 zmiennych ilościowych oraz z 5 jakościowych.  W tym podrozdziale najpierw zostanie sprawdzony typ danych, podstawowe statystyki, występowanie brakujących obserwacji oraz odstających, a następnie przekształcenie ich oraz obróbka zmiennych jakościowych.

## Missing data

Poniższy wykres przedstawia w jakich kolumnach oraz z jaką częstotliwością występują brakujące dane w zbiorze danych.
"""

ax = sns.heatmap(df.isnull()).set_title("Wykrywanie brakujących wartości")

"""W tym zbiorze danych w aż 7 kolumnach występują brakujące wartości. 
Istnieje wiele sposobów radzenia sobie z brakujacymi danymi tzw. **missing data**. Techniki te najprościej podzielić na takie w których usuwa się dane (pojedyncze obserwacje lub całe kolumny) oraz na takie, w których imputuje się dane (ang. "impute"). W zależności od tego z jakimi danymi mamy doczynienia, czy są to szeregi czasowe, czy klasyczne dane, czy ciągłe, czy binarne itd. dobieramy odpowiednie metody.

Gender - płeć jest pierwszą kolumną w której występują brakujące dane. Ze względu na małą ilość brakujących danych (12) decyduję się je usunąć. Analogicznie Married. Self_Employed jest zmienną która zawiera wiele brakujących danych. Z tego względu decyduje się nie usuwać tych wartości.

Dla pozostałych 3 kolumn, które miały wyraźnie wiecej obserwacji brakujacych niż inne stosuje statystyczną strategie polegającą na uzupełnieniu danych najpopularniejszą wartościa dla danej zmiennej objaśniającej. Jest to prosty, ale skuteczny sposób radzi sobie zarówno ze zmiennymi ciągłymi jak i kategorycznymi.
"""

#usuwanie brakujących wartości
df = df.dropna(subset= ["Gender","Married","Dependents", "Loan_Amount_Term"]) 
# zastepowanie brakujacych wartosco najczesciej wystepujacymi
imp_mean = SimpleImputer(strategy='most_frequent')
imp_mean.fit(df)
imputed_data_india = imp_mean.transform(df)
imputed_data_india = pd.DataFrame(imputed_data_india, columns = col_names)
imputed_data_india['Dependents'] = imputed_data_india['Dependents'].replace(['3+'], 3)
imputed_data_india['Dependents'] = imputed_data_india['Dependents'].astype(int)
df = imputed_data_india
#Konwersja typów
df['Gender'] = df['Gender'].astype(str)
df['Married'] = df['Married'].astype(str)
df['Education'] = df['Education'].astype(str)
df['Self_Employed'] = df['Self_Employed'].astype(str)
df['ApplicantIncome'] = df['ApplicantIncome'].astype(int)
df['CoapplicantIncome'] = df['CoapplicantIncome'].astype(int)
df['LoanAmount'] = df['LoanAmount'].astype(int)
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype(int)
df['Credit_History'] = df['Credit_History'].astype(int)
df['Property_Area'] = df['Property_Area'].astype(str)
df['Loan_Status'] = df['Loan_Status'].astype(str)

"""Po usunięciu brakujących obserwacji z 614 obserwacji pozostało ich 573.

## Statystyki opisowe

**Dane ilościowe:**

W zaprezentowanym zbiorze danych średnio przypada 0,7 osoby będącej na utrzymaniu kredytobiorcy. Średni dochód wynosi ok 5300 dolarów. Średni przychód współaplikanta wynosi ok 1640 dolarów. Średnia wysokość kredytu wynosi 145$, a zapadalność kredytu 341 dni.

**Dane jakościowe:**  

Poniżej tworzę funkcję **distribution_plot** przedstawiającą rozkład zmiennych jakościowych:
"""

def distribution_plot(df, col_name, title=''):
    labels = df[col_name].unique()
    for i in labels:
        x = df[df[col_name] == i]
        x = len(x)/len(df)
        print(i,":",x*100,'%')
    labels = labels.sort() 
    classes = pd.value_counts(df[col_name], sort = True)
    classes.plot(kind = 'bar', rot=0, color='#701118')
    title = str(col_name) + " distribution" 
    plt.title(title)
    plt.xticks(range(4), labels)
    plt.ylabel("Frequency")
    plt.show()
    
for col in col_names[1:5]:
    distribution_plot(df, col)
for col in col_names[9:len(df)]:
    distribution_plot(df, col)

"""**Wnioski**:
W zbiorze danych przeważając mężczyźni 80%, w większosci sa to osoby zameżne 65%, w 78% są to osoby wykształcone, i w 86% są to osoby nie prowadzące samodzielnej działalnosci gospodarczej. W zbiorze danych jest mniej wiecej podobna proporcja osob mieszkajacych w warunkach miejskich, podmiejskich i wiejskich. Przeważają jednak osoby zamieszkujace podmiejskie obszary 38%.
W prawie 70% przypadków kredyty są przyznane.

## Korelacja zmiennych

Zmienne objaśniające nie powinny być wzajemnie ze sobą skorelowane, stąd przedstawiam poniżej macierz korelacji. Ze względu na obecność wartości odstających w zbiorze danych stosuję **korelację Spearmana**, która jest odporna na obecność wartości odstających.
"""

# heat map of correlation of features
#correlation_matrix = df.iloc[:,0:(df.shape[1]-1)].corr(method = "spearman")
correlation_matrix = df.iloc[:,0:(df.shape[1])].corr(method = "spearman")
ax = sns.heatmap(correlation_matrix,vmax=0.9,square = True, annot=True,  annot_kws={"size": 10} ).set_title("Macierz korelacji Spearmana")

dummy_vars = ['Dependents','Property_Area']
for var in dummy_vars:
    dummy_list='var'+'_'+var
    dummy_list = pd.get_dummies(df[var], prefix=var)
    data1=df.join(dummy_list)
    df=data1
data_vars=df.columns.values.tolist()
to_keep=[i for i in data_vars if i not in dummy_vars]
data_final=df[to_keep]
data_final['Married'] = data_final['Married'].map({'Yes': 1, 'No': 0})
data_final['Gender'] = data_final['Gender'].map({'Female': 1, 'Male': 0})
data_final['Education'] = data_final['Education'].map({'Graduate': 1, 'Not Graduate': 0})
data_final['Self_Employed'] = data_final['Self_Employed'].map({'Yes': 1, 'No': 0})
data_final['Loan_Status'] = data_final['Loan_Status'].map({'Y': 1, 'N': 0})
columns = list(data_final.columns.values)
columns.remove('Loan_Status')
columns.append('Loan_Status')
data_final = data_final[columns]
data_final.head()

"""## Outliers

Kolejnym istotnym etapem data preprocessingu jest wykrycie wartości odstających. Najpierw wykrywam je poprzez stworzenie wykresów pudełkowych.
"""

sns.boxplot(x=data_final['ApplicantIncome'],color="pink")
plt.show()
sns.boxplot(x=data_final['CoapplicantIncome'], color="pink")
plt.show()
sns.boxplot(x=data_final['LoanAmount'],color="pink")
plt.show()

print("Ilosc danych przed usunieciem wartosci odstajacych: " + str (data_final.shape[0]))
data_final = data_final[(np.abs(stats.zscore(data_final[["ApplicantIncome","CoapplicantIncome","LoanAmount"]])) < 3).all(axis=1)]
print("Ilosc danych po usunieciu wartosci odstajacych: " +str(data_final.shape[0]))

"""# Podział na zbiór testowy oraz treningowy

W tej części zostanie przedstawiona implementacja podziału zbioru danych na testowy oraz treningowy oraz walidacji krzyżowej. Te dwa działania są istotnym aspektem Data Science i analizy danych, ponieważ milimalizują ryzyko nadmiernego dopasowania tzw. overfittingu modelu.

W machine learningu oraz statystyce przeważnie dokonuje się podziału zbioru danych na testowy i treningowy.  Jeżeli wystąpi zjawisko overfittingu oznacza to, że model jest za bardzo wytrenowany i daje odpowiednie predykcje wyłącznie w zbiorze treningowym.
Zazwyczaj dzieląc zbiór korzysta się z proporcji 80/20 lub 70/30 obserwacji treningowych do testowych.

## Dobór zmiennych

Dobieram zmienne do modelu stosując funkcje RFE dla regresji logistycznej. 
Zmienne objaśniające użyte do budowy modeli to:

*   Married
*   Education
*  Credit History
*  Dependents_2
*  Property_Area_Semiurban


Dla pozostałych technik pozostawiam kompletny zbiór bez usuwania zmiennych objaśniających.
"""

data_final_vars=data_final.columns.values.tolist()
X = data_final.loc[:, data_final.columns != 'Loan_Status'].values
y = data_final.loc[:, data_final.columns == 'Loan_Status'].values

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(solver='lbfgs',max_iter=1000)
rfe = RFE(logreg,5)
rfe = rfe.fit(X, y.ravel())
print(rfe.support_)
print(rfe.ranking_)
cols=['Married','Education','Credit_History','Dependents_2','Property_Area_Semiurban']
X = data_final[cols].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 222)

"""# Budowa i ocena modeli

 70 % problemów dziedziny Data Science polega na rozwiązaniu problemu klasyfikacji. Poniżej zostaną zbudowane 3 modele różnymi technikami.
 Jakość i poprawność modeli klasyfikacyjnych zostanie oceniona na podstawie: 
 
 *  **Tablica pomyłek**  
 
 Tablica pomyłek (macierz błędów) jest stosowana przy ocenie jakości klasyfikacji binarnej (na dwie klasy). Dane oznaczone etykietami: pozytywną i negatywną poddawane są klasyfikacji, która przypisuje im predykowaną klasę pozytywną albo predykowaną klasę negatywną. Możliwa jest sytuacja, że dana oryginalnie oznaczona jako pozytywna zostanie omyłkowo zaklasyfikowana jako negatywną. Wszystkie takie sytuację przedstawia tablica pomyłek.  
 
Do oceny modelu posłużą dwie miary: czułość (ang. recall), czyli odsetek prawdziwie pozytywnych oraz dokładność.

* **F1 score**
To miara będąca funkcją precyzji oraz czułości miar pochodzących z tablicy pomyłek. Pomaga ona zbadać proporcję pomiędzy tymi dwoma wartościami. F1 przyjmuje wartości z przedziału od 0 do 1 i im F1-score bliższy 1 tym lepszy jest model.

 
* **AUC** jest to pole pod krzywą ROC  (Area Under ROC Curve). Natomiast krzywa ROC to  jeden ze sposobów wizualizacji jakości klasyfikacji, pokazujący zależności wskaźników TPR (True Positive Rate) oraz FPR (False Positive Rate). Wartość AUC pozwala ocenić jakość predykcji. Im większe AUC tym lepiej: AUC = 1 (klasyfikator idealny), AUC = 0.5 (klasyfikator losowy), AUC < 0.5 (nieprawidłowy klasyfikator (gorszy niż losowy)).

* **Krzywa ROC** wyznaczana jest dla wszystkich możliwych punktów odcięcia.  **Czułość**, czyli True Positive Rate opisuje częstość wystąpień prawdziwie dodatnich. Natomiast **Swoistość**, czyli True Negative Rate opisuje częstość względną wystąpień fałszywie dodatnich.

Poniżej zostały stworzone 2 funkcje, które będą służyły do wizualizacji wyników jakości modeli. Pierwsza funkcja tworzy tablicę pomyłek i wyświetla podstawowe statystyki, takie jak accuracy, czy f1-score. Druga funkcja tworzy krzywą roc oraz oblicza AUC.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc


def wyswietl_tablice_pomylek(predictions):
  print(classification_report(y_test, predictions))
  array = confusion_matrix(y_test, predictions)      
  df_cm = pd.DataFrame(array, range(2),range(2))
  plt.figure(figsize = (6,4))
  sns.set(font_scale=1.4)#for label size
  sns.heatmap(df_cm, annot=True).set_title("Tablica pomylek")

def roc_auc_plot(clf, type):
  y_scores = clf.predict_proba(X_test)
  fpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])
  roc_auc = auc(fpr, tpr)
  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
  plt.legend(loc = 'lower right')
  plt.plot([0, 1], [0, 1],'r--')
  plt.xlim([0, 1])
  plt.ylim([0, 1])
  plt.ylabel('Czułość')
  plt.xlabel('Swoistość')
  plt.title('Krzywa ROC dla ' + type)
  plt.show()

"""## Regresja logistyczna

Regresja logistyczna jest to model stosowany w uczeniu maszynowym do klasyfikacji. Model regresji logistycznej prognozuje prawdopodobieństwo wystąpienia zjawiska jako funkcję zmiennych niezależnych. Jest to uogólnienie regresji stosowane wtedy, kiedy zmienna zależna przyjmuje 2 wartości.

Właściwości regresji logistycznej:

*  Zmienna zależna należy do rozkładu zero-jedynkowego tzw. Bernoilliego.

* Estymatory są estymowane metodą największej wiarygodności.

* Nie uwzględnia się współczynnika R-kwadrat, model jest dopasowany przez
"""

log_model = LogisticRegression(solver='lbfgs').fit(X_train, y_train.ravel())
log_y_pred = log_model.predict(X_test)
#wizualizacja do oceny modelu   
roc_auc_plot(log_model,"regresji logistycznej")
wyswietl_tablice_pomylek(log_y_pred)

"""**Wnioski Regresja Logistyczna**  

Zbudowany model regresji charakteryzuje się accuracy na poziomie 81%, zatem jakość klasyfikacji szacuje się na poziomie 81%.
Współczynnik f1 wynosi 87% dla przyjętych wniosków i 64% dla odrzuconych. 
AUC wynosi  0.81, oznacza to, że klasyfikator działa poprawnie.

**Przygotowanie danych**

Dla KNN i SVM tworzę na nowo zbiór testowy oraz treningowy bez eliminacji zmiennych.
Dodatkowo standaryzuje zmienne X.

## Algorytm K-najbliższych sąsiadów

Algorytm k-NN jest nieparametryczną metodą stosowaną w klasyfikacji oraz regresji. Jest to przykład uczenia nadzorowanego. K-NN cechuje prostota implementacji w podstawowej formie. 

Algorytm k_NN oblicza odległość pomiędzy nowym punktem danych, a współrzędnymi danych ze zbioru treningowego. Odległość może być mierzone odległością euklidesową lub Manhattan. Każdej danej należy przypisać pewien zestaw n cechujących ją wartości, a następnie umieścić ją w n-wymiarowej przestrzeni. Chcąc przyporządkować daną do już istniejącej grupy, należy znaleźć k najbliższych obiektów w przestrzeni n-wymiarowej, a następnie wybrać grupę najbardziej liczną.
"""

#dla KNN dla SVM
from sklearn.preprocessing import StandardScaler
X = data_final.loc[:, data_final.columns != 'Loan_Status'].values
y = data_final.loc[:, data_final.columns == 'Loan_Status'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 222)  
scalerX = StandardScaler().fit(X_train)
X_train = scalerX.transform(X_train)
X_test = scalerX.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
classifier_knn = KNeighborsClassifier(n_neighbors=15)
classifier_knn.fit(X_train, y_train.ravel())  
y_pred_knn = classifier_knn.predict(X_test)
#wizualizacja do oceny modelu   
roc_auc_plot(classifier_knn,"KNN")
wyswietl_tablice_pomylek(y_pred_knn)

"""**Wnioski KNN**  
AUC wynosi 0.75 co oznacza, że model jest przeciętny tzn. lepszy od klasyfikatora losowego, ale nie klasyfikuje poprawnie wszytskich wyników. 
Accuracy wynosi 72% co również wskazuje na przeciętność prognoz.
Współczynniki f-1 score pokazują, że model lepiej radzi sobie z klasyfikacją przyjętych wniosków.

## SVM

Supprot vector machine, czyli maszyna wektorów nośnych.  Ma za zadanie jak najlepiej odseparować od siebie elementy różnego typu ze zbioru uczącego, czyli umieścić je w optymalnie oddzielonych grupach.

Support Vector Machine cechuje się, tym że próbuje oddzielić dane funkcją liniową, a jeśli to nie wychodzi przenosi dane do wyższych wymiarów i  wtedy jeszcze raz próbuje znaleźć taką funkcję.

Zalety SVM:
* Znajduje maksymalne odległości (marginesy) pomiędzy grupami punktów
* Efektywne obliczeniowo - złożoność rośnie tylko liniowo wraz z liczbą wymiarów
* Rozwiązuje problemy liniowe jak i nieliniowe

Dla modelu SVM stosuję jądro(kernel) w postaci funkcji RBF, czyli radial basis function kernel.
"""

from sklearn import svm
from sklearn import metrics

classifier_svm = svm.SVC(kernel='rbf',probability=True) # Linear Kernel
classifier_svm.fit(X_train, y_train.ravel())
y_pred_svm = classifier_svm.predict(X_test)

#wizualizacja do oceny modelu   
roc_auc_plot(classifier_svm,"SVM")
wyswietl_tablice_pomylek(y_pred_svm)

"""**Wnioski SVM** 

Model SVM charakteryzuje sie dokładnością na poziomie 81%. Lepiej klasyfikuje wnioski przyznane niż odrzucone podobnie jak dwa powyższe modele.
AUC wynosi 0,78 co oznacza, że klasyfikator działa dobrze.

# Podsumowanie 

Stworzone modele działają poprawnie. Niewątpliwie ma na nie wpływ mała liczba dostępnych obserwacji w próbce.  Podczas przygotowywania projektu została podjęta próba over-samplingu metodą SMOTE, aby powiększyć próbę. Rozwiązanie to jednak nie przyniosło oczekiwanych rezultatów wyniki modelu wcale nie uległy poprawie.

Preprocessing danych składał się z usunięcia obserwacji brakujących w przypadku, kiedy nie było ich zbyt dużo oraz na zastosowaniu imputacji zatępując ich wartość wartością najczęsciej występującą w kolumnie.
Ponad to dane odstające zostały usunięte.

Zbiór testowy oraz treningowy został podzielony korzystając z proporcji 30/70.

Podsumowując wszystkie 3 algorytmy działają podobnie. Największe AUC wykazuję regresja logistyczna, które wynosi 0.81 zatem klasyfikator działa dobrze. 
Natomiast największe accuracy wykazały na takim samym poziomie 81% model regresji logistycznej oraz SVM.
"""